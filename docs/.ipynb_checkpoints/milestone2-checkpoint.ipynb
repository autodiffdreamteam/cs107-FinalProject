{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Differentiation \n",
    "Differentiation is one of the most commonly used mathematical operations. Fundamentally, it computes the rate of change of a variable with respect to another. This rate is referred to as the derivative of the first variable with respect to the second. Differentiation generalizes from single to multi-valued functions, for which we can compute derivatives with respect to one or more dependent variables. Values of these derivatives serve as critical inputs to many numerical algorithms (e.g. Newton's method). Given the rapid growth of complexity in modern computational problems, it is imperative to compute these derivatives both quickly and accurately. \n",
    "\n",
    "## Automatic differentiation (AD)\n",
    "Automatic Differentiation (abbreviation: AD and also referred to as Algorithmic or Computational Differentiation) is a method to evaluate derivatives of real-valued functions. It is a variant of the classically conceptualized computer-based differentiation methods such as symbolic differentation and finite difference methods but addresses shortcomings encountered in both. For example, symbolic differentation possesses the capability to compute derivatives to machine precision, however the required computational times can be quite large. On the other hand, finite difference differentiation is quicker but produces errors in the computed derivatives that are several order of magnitudes higher than machine precision. AD emerges as a solution devoid of shortcomings observed in both symbolic and finite difference methods and hence is gaining popularity in computational scientific applications.\n",
    "\n",
    "## Applications of automatic differentiation\n",
    "Automatic differentiation can be applied to many important computational problems. Examples include root finding methods such as the Newton's method, optimization schemes such as the Gradient Descent method, and machine learning algorithms such as the backpropagation algorithm. All these methods have a critical dependence on fast and accurate computation of derivates. Automatic differentiation serves these requirements and hence is an appropriate choice for calculating the derivatives for these algorithms.\n",
    "\n",
    "## The gradient descent method (GDM)\n",
    "The Gradient Descent Method (GDM) is an iterative optimization algorithm. It can be used to find local minima of a differentiable function by iteratively miniminizing a cost function and ultimately stopping at the minimum value of the cost function, which indicates a local minimum.\n",
    "\n",
    "In 1-D the following steps are performed in GDM.\n",
    "**Step 1**: Given a function in $x$, start at a random point $x_0$\n",
    "\n",
    "**Step 2**: Compute the derivative at $x_0$\n",
    "\n",
    "**Step 3**: Check if the algorithm is going in the right direction based on the sign of the derivative or if it has reached the minimum\n",
    "\n",
    "**Step 4**: Get a new value of $x$ and repeat Steps 1-3 until convergence to a local minimum\n",
    "\n",
    "This approach can be extended to higher dimensions.\n",
    "\n",
    "## Scope\n",
    "The software package presented here provides a computational tool to compute derivatives of multi-valued functions employing the AD technique. The package will compute derivatives using forward AD method. In addition, it will enable optimization using the GDM. \n",
    "\n",
    "\n",
    "## References\n",
    "1. Hoffmann, P. H. (2016). A hitchhiker’s guide to automatic differentiation. Numerical Algorithms, 72(3), 775-811.\n",
    "2. van Merrienboer, B., Moldovan, D., & Wiltschko, A. (2018). Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming. In Advances in Neural Information Processing Systems (pp. 6256-6265).\n",
    "3. https://harvard-iacs.github.io/2020-CS107/lectures/.\n",
    "4. Griewank, A. and Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation (Vol. 105). Siam.\n",
    "5. Nocedal, J. and Wright, S., 2001. Numerical Optimization, Second Edition. Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## The chain rule\n",
    "\n",
    "Automatic differentitation is built upon the chain rule, which provides a formula to compute the derivative of composite functions. Recall that if we have a composite function\n",
    "\n",
    "$$ f(g(h(x))), $$\n",
    "\n",
    "the chain rule tells us that\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial h}\\frac{\\partial h}{\\partial t}. $$\n",
    "\n",
    "In this package, we implement the forward mode of AD, whereby we evaluate the derivative of a composite function from the inside out. Put differently, we differentiate with respect to one independent variable and recursively find the derivative of each sub-expression starting with the innermost function. In general, the forward mode would evaluate the derivative above as follows:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial v_{n-1}} \\frac{\\partial v_{n-1}}{\\partial x} = \\frac{\\partial f}{\\partial v_{n-1}}  \\left( \\frac{\\partial v_{n-1}}{\\partial v_{n-2}}\\frac{\\partial v_{n-2}}{\\partial x} \\right) = \\frac{\\partial f}{\\partial v_{n-1}} \\frac{\\partial v_{n-1}}{\\partial x} = \\frac{\\partial f}{\\partial v_{n-1}}  \\left( \\frac{\\partial v_{n-1}}{\\partial v_{n-2}} \\left( \\frac{\\partial v_{n-2}}{\\partial v_{n-3}} \\frac{\\partial v_{n-3}}{\\partial x} \\right) \\right) = ..., $$\n",
    "\n",
    "where each $ v_i $ is an inner function. If the function we want to differentiate has multiple arguments, as in\n",
    "\n",
    "$$ f(g(t), h(t)), $$\n",
    "\n",
    "then the chain rule extends naturally, giving\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x} + \\frac{\\partial f}{\\partial h}\\frac{\\partial h}{\\partial x}. $$\n",
    "\n",
    "And if we have a vector-valued function with $ x \\in \\mathbb{R}^m $\n",
    "\n",
    "$$ f = f(g(x), h(x)), $$\n",
    "\n",
    "the derivative becomes\n",
    "\n",
    "$$ \\nabla_x f = \\frac{\\partial f}{\\partial g} \\nabla g + \\frac{\\partial f}{\\partial h} \\nabla h. $$\n",
    "\n",
    "Putting this all together, we can use the chain rule to express the derivative of a general vector-valued function $ f=f(y(x)) $ where $ y \\in \\mathbb{R}^n $ and $ x \\in \\mathbb{R}^m $ as\n",
    "\n",
    "$$ \\nabla_x f = \\sum_{i=1}^n \\frac{\\partial f}{\\partial y_i} \\nabla y_i(x). $$\n",
    "\n",
    "\n",
    "## The computational graph\n",
    "\n",
    "Consider the function $ f(x, y) = e^{x^2 + y^2}. $ For illustrative purposes, we can summarize the recursive operations of forward mode AD in a computational graph. \n",
    "\n",
    "Let's say we want to find $ \\partial f / \\partial x. $ Because we are differentiating with respect to $ x, $ we start by calculating seed values as:\n",
    "\n",
    "$$ \\frac{\\partial x}{\\partial x} = 1, $$ and\n",
    "$$ \\frac{\\partial y}{\\partial x} = 0. $$\n",
    "\n",
    "Then the derivative is evaluated as follows:\n",
    "\n",
    "| Trace   | Elementary Operation   | Derivative Operation                     |\n",
    "|---------|------------------------|------------------------------------------|\n",
    "| $v_1$   | $x$                    | $\\dot{v_1}=1$ (seed)                     |\n",
    "| $v_2$   | $v_1^2$                | $\\dot{v_2}=2v_1 \\dot{v_1}$               |\n",
    "| $v_3$   | $y$                    | $\\dot{v_3}=0$ (seed)                     |\n",
    "| $v_4$   | $v_3^2$                | $\\dot{v_4}=2v_3 \\dot{v_3}$               |\n",
    "| $v_5$   | $v_2 + v_4$            | $\\dot{v_5}=\\dot{v_2} + \\dot{v_4}$        |\n",
    "| $f$     | $e^{v_5}$              | $\\partial f/\\partial x=\\dot{v_5}e^{v_5}$ |\n",
    "\n",
    "In essence, forward mode AD does computationally what we did by hand in the table above. In general, it computes the product $\\nabla f \\cdot p,$ where $ p $ is a seed vector computed as we did in the example above. If $ f $ is a vector-valued function, then it computes $ Jp, $ where $ J $ is the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use\n",
    "\n",
    "In its current state, the software does not need installation. It is contained within AutoDiff.py and uses the AutoDiffPy class to perform forward mode automatic differentiation. For now, the user should `git clone` into our project repo in order to access and import this class. In addition, the user should also install the dependencies listed in `requirements.txt`.\n",
    "\n",
    "## Installation \n",
    "1. Running this application requires the Python language. Please download it from: https://www.python.org/downloads/ depending on the operating system.\n",
    "2. Run the installation process and set up the Python Path correctly on your system so that you can access it from the terminal (Windows command prompt or bash shell in MAC OS).\n",
    "3. Type `python` on your system shell (Windows command prompt or bash shell in MAC OS) to start the Python runtime environment or you can use any of the graphical user interfaces such as Visual Studio, or Python IDLE to start running Python.\n",
    "4. `git clone https://github.com/autodiffdreamteam/cs107-FinalProject.git` and make sure the AutoDiff.py class is in your working directory.\n",
    "5. Create your Python script to run.\n",
    "6. Install the dependencies in `requirements.txt` including `NumPy`, `math` and `pytest` (if running testing tasks).\n",
    "7. Run the program. Instructions on how to do this are provided below.\n",
    "\n",
    "## Basic demo\n",
    "\n",
    "Please see the \"implementation\" section for a more detailed explanation of the multiple ways in which users can interact with `AutoDiffPy`.\n",
    "\n",
    "1. Function to differentiate: $\\sin(x)$. Declare this function as a string. \n",
    "\n",
    "```\n",
    "inp_function = \"sin(x)\"\n",
    "```\n",
    "\n",
    "2. Import the AutoDiffPy class into your Python script. \n",
    "\n",
    "```\n",
    "from AutoDiff import AutoDiffPy as ad\n",
    "```\n",
    "\n",
    "3. Initialize the point at which to evaluate. \n",
    "\n",
    "```\n",
    "a = math.pi/2.0\n",
    "```\n",
    "\n",
    "4. Pass the function and the point of evaluation to the AutoDiffPy class to create an AutoDiff object. \n",
    "\n",
    "```\n",
    "returned_result = ad(a, input_function = inp_function)\n",
    "```\n",
    "\n",
    "5. Print out the result by accessing the properties of the `returned_result` object.\n",
    "\n",
    "```\n",
    "print(returned_result.val) # for the function value\n",
    "print(returned_result.der) # for the function derivative\n",
    "```\n",
    "\n",
    "\n",
    "# Software Organization\n",
    "\n",
    "## Directory structure\n",
    "Provided below is a proposed directory structure for the AD library.\n",
    "\n",
    "```\n",
    "cs107-FinalProject/\n",
    "        README.md\n",
    "        requirements.txt\n",
    "        docs/\n",
    "            documentation.ipynb\n",
    "        ad_library/\n",
    "            AutoDiff.py\n",
    "            GradDesc.py\n",
    "            AutoDiffTest.py\n",
    "```\n",
    "\n",
    "## Python modules\n",
    "See `requirements.txt` for a list of dependencies. These include:\n",
    "- `NumPy` for performing elementary mathematical operations.\n",
    "- `math` for accessing mathematical constants and some basic functions.\n",
    "- `pytest` for testing.\n",
    "\n",
    "\n",
    "## Test suite\n",
    "The package will be tested using the TravisCI testing tool, and all tests can be found in `AutoDiffTest.py`. They can be used simply by running any of the provided functions, e.g. `test_case1()`.\n",
    "\n",
    "## Package distribution\n",
    "Currently, our package is available only through GitHub and can be accessed by following the installation instructions above. In a future milestone, however, we will make our library available through `PyPI`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Data structures\n",
    "\n",
    "Our current implementation of `AutoDiffPy` makes use of a broad range of data structures, including: \n",
    "- `NumPy` arrays for optimized storage and fast retrieval of data \n",
    "- Tuples to conveniently store pairs of values, for example `(f.val, f.der)` \n",
    "- Dictionaries, for example to map elementary function strings (e.g. `\"sin\"`) to class functions (e.g. `self.sin`)\n",
    "\n",
    "## `AutoDiffPy`\n",
    "\n",
    "`AutoDiffPy` can now be constructed in two different ways, based on the optional `input_function` parameter in the `__init__` dunder method:\n",
    "\n",
    "- **Option 1**: Use the `val` and optionally the `der` parameters.\n",
    "This option allows for broadly the most flexible usage and allows users to access the elementary operations and mathematical functions built into the `AutoDiffPy` class. For example, the user could print out the value and the derivative of $f(x) = \\cos(\\sin(x))$ at $x=2.0$ simply by doing:\n",
    "\n",
    "```\n",
    "a = 2.0\n",
    "f = ad(a)\n",
    "f = ad.sin(f)\n",
    "f = ad.cos(f)\n",
    "print(f.val, f.der)\n",
    "```\n",
    "\n",
    "- **Option 2**: Use the optional `input_function` parameter.\n",
    "The second, more exciting approach (which we belive sets our AD library apart from others), is to pass a function string to the optional `input_function` parameter, which enables the user to evaluate the derivative of complex functions without having to construct those functions manually, as shown above. The example above then becomes:\n",
    "\n",
    "```\n",
    "a = 2.0\n",
    "f = ad(a, input_function=\"cos(sin(x))\")\n",
    "print(f.val, f.der)\n",
    "```\n",
    "\n",
    "For simple functions such as the one in this example, the advantage of the second option is trivial. But for more complex functions, this method is both easier and less error-prone.\n",
    "\n",
    "### `input_function` syntax\n",
    "`input_function` is a string representing some expression. There are a few syntactic rules to be followed in order to ensure proper parsing.\n",
    "\n",
    "1. `x` represents the variable\n",
    "    - Currently can only handle `x` as a variable to be evaluated at value.\n",
    "\t- All `input_function` must contain at least one `x`\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "log(5*x)\n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "log(5*y)\n",
    "```\n",
    "\n",
    "2. `*` represents multiplication\n",
    "    - This is required for variable coefficients.\n",
    "    - If used as an operator besides as a coefficient, must have a space on its left and right.\n",
    "    - Used only inside the center polynomial\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "3*x * x\n",
    "3 * x^5\n",
    "3 * 2*x^4\n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "3*x*x\n",
    "3*x**5\n",
    "```\n",
    "\n",
    "3. `/` represents division\n",
    "    - Can't be used directly for variable coefficients. Must be written left to right with proper `*` multiplier. \n",
    "    - If being used to operate besides as a coefficient, must have a space on its left and right.\n",
    "    - Used only inside the center polynomial\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "1/3*x^5\n",
    "log(3*x^4 / 2*x^5)\n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "1/x^5\n",
    "2/ log(x^5)\n",
    "```\n",
    "\n",
    "4. `^` represents an exponent term to follow\n",
    "    - This symbol follows an `x`\n",
    "    - It represents that variable `x` will be to the power of the following numeric value\n",
    "    - Used only inside the center polynomial\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "3*x^5\n",
    "log(x^4)\n",
    "log(3*x^2 + 2*x^3)\n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "log(x + 3^x)\n",
    "sin(x + 3^4)\n",
    "```\n",
    "\n",
    "5. Even parentheses on left and right of polynomial\n",
    "    - The polynomial containing at least one `x` term must have an even number of parentheses surrounding it.\n",
    "    - There can be no additional parentheses in the center polynomial\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "3*x\n",
    "log(3*x)\n",
    "sin(log(cos(3*x^5 + x)))\n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "sin(log(3*x + (5))))\n",
    "sin(log(cos(3*x)\n",
    "```\n",
    "\n",
    "6. No terms or operations to the right of right parentheses\n",
    "    - Any terms or operations to the right of the first right parentheses will be ignored.\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "log(sin(3*x^4 + 15))\n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "log(sin(3*x^4)^5) * log(x)\n",
    "log(3*x)^4\n",
    "```\n",
    "\n",
    "7. Single polynomial expression on inside of parentheses\n",
    "    - In the centermost parentheses, it can only be a polynomial that uses elementary operations\n",
    "    - You can’t place operations like `log` or `cos` inside of the center polynomial\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "log(log(sin(3*x + 5*x^2)))\n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "log(sin(x) + cos(3*x^2))\n",
    "```\n",
    "\n",
    "8. `+` and `-` can be used inside polynomial\n",
    "    - `+` and `-` represent plus and minus respectively and can be used between terms, whether constant or `x` valued.\n",
    "\n",
    "Proper Usage:\n",
    "```\n",
    "3*x + 5\n",
    "log(3*x + 5-4)\n",
    "log(3*x^4+5 - x^4) \n",
    "```\n",
    "Improper Usage:\n",
    "```\n",
    "log(3*x) + log(2*x)\n",
    "```\n",
    "\n",
    "9. Expression inside center polynomial will be evaluated left to right, using EMDAS, (excluding P because parentheses are not allowed inside center expressions)\n",
    "\n",
    "Hence, the general form by concatenation is:\n",
    "```\n",
    "function(\"^n\" + \"polynomial\" + \")\"^n\n",
    "```\n",
    "where both the whole expression and the polynomial on the inside follow rules 1-9.\n",
    "\n",
    "Examples of valid `input_function`, `input_function =:`\n",
    "```\n",
    "\"log(sin(cos(x)))\"\n",
    "\"log(x^2)\"\n",
    "\"tan(3*x^5)\"\n",
    "\"tan(3*x^5 + 10)\"\n",
    "\"sin(2*x -15)\"\n",
    "\"cos(2*x +15 - 15)\"\n",
    "\"log(sin(2*x^4 * 2*x^2))\"\n",
    "\"cos(2*x^6 / 5*x)\"\n",
    "```\n",
    "\n",
    "Examples of invalid `input_function`, `input_function =:`\n",
    "```\n",
    "\"log(5) + log(x)\"\n",
    "\"log(5*x)^5\"\n",
    "\"log + 5x\"\n",
    "\"log(5*x) * 5\"\n",
    "\"log(5*x**3)\"\n",
    "\"sin(3*x*x*x)\"\n",
    "```\n",
    "\n",
    "\n",
    "## `GradDescPy`\n",
    "This library will be implemented in a future milestone, but we anticipate including the following attributes and methods.\n",
    "\n",
    "1. Attributes:\n",
    "    - `params`\n",
    "        - `params` contains vector of coefficients\n",
    "    - `optimization_function`\n",
    "        - The optimization function for gradient descent\n",
    "    - `cost_function_value`\n",
    "        - The value of the cost function\n",
    "\n",
    "2. Methods:\n",
    "    - `__init__(parameters, optimization_function = default_of)`\n",
    "        - Parameters: `parameters` which includes an array representing the vector, and optional parameters which define the optimization function\n",
    "        - Returns: A constructed `ad_optimizer` object to perform optimization via gradient descent.\n",
    "    - `parse_function(user_input)`\n",
    "        - Parameters: Raw user input representing a function.\n",
    "        - Returns: The parsed function to input into the optimize function \n",
    "    - `optimizer(vector)`\n",
    "        - Performs gradient descent given a cost function and a set of vectors\n",
    "    - `get_params()`\n",
    "    - `set_params()`\n",
    "\n",
    "\n",
    "## External dependencies\n",
    "See `requirements.txt`\n",
    "\n",
    "## Elementary functions\n",
    "In order to allow users to construct and compute the derivatives of complex functions, the `AutoDiffPy` class overloads all the basic mathematical operations, including:\n",
    "```\n",
    "__add__\n",
    "__radd__\n",
    "__sub__\n",
    "__rsub__\n",
    "__mul__\n",
    "__rmul__\n",
    "__truediv__\n",
    "__rtruediv__\n",
    "__pow__\n",
    "__rpow__\n",
    "__neg__\n",
    "```\n",
    "\n",
    "In addition to the operator overloading, the `AutoDiffPy` class includes common elementary functions, including:\n",
    "```\n",
    "AutoDiffPy.exp(x)\n",
    "AutoDiffPy.log(x)\n",
    "AutoDiffPy.sin(x)\n",
    "AutoDiffPy.cos(x)\n",
    "AutoDiffPy.tan(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future features\n",
    "While our current implementation works for a broad range of use cases, there are a number of logical extensions to `AutoDiffPy` that we will be pursuing in future milestones. These include:\n",
    "1. Enable vector-valued function inputs: \n",
    "    - In the next milestone, we plan on implementing forward mode AD with vector-valued functions.\n",
    "    - As we did in this milestone, we plan on providing users with the option of construction those functions manually, or by using the `input_function` function, which will ultimately be able to identify the number of variables, decide whether it is a vector, and parse it appropriately.\n",
    "\n",
    "\n",
    "2. Make the `input_function` more flexible: as seen in the `input_function` syntax section, our current implementation of `parse_input` is somewhat rigid in places. In the next milestone, we plan on allowing for \n",
    "    - parentheses within the center polynomial,\n",
    "    - power outside of parentheses, and\n",
    "    - operations between functions\n",
    "\n",
    "\n",
    "3. Implement the `GradDescPy` class:\n",
    "    - To increase the usefulness of our `ad_library`, we will eventually implement a class that uses the `AutoDiffPy` class to perform gradient descent.\n",
    "    - We plan on achieving consistency between the structure of `AutoDiffPy` and `GradDescPy` so that these two classes work well in tandem. In particular, we hope that the `input_function` option within `AutoDiffPy` will allow users to seamlessly type in a function and easily perform an optimization on it.\n",
    "    - As we did in `AutoDiffPy`, we will strive to ask the user for minimal inputs, abstracting the interface as necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
