{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Importance of Differentiation \n",
    "Differentiation is one of the most commonly used mathematical operations. Fundamentally, it computes the rate of change of a variable with respect to another. This rate is referred to as the derivative of the first variable with respect to the second. Differentiation is also applied to single or multi-valued functions to compute their derivatives with respect to one or more dependent variables. Values of these derivatives serve as a critical input to several numerical algorithms (e.g. Newton's method) and considering the fast growing complexity of the computational problems being solved these days it is imperative to compute these derivatives accurately as well as quickly. \n",
    "\n",
    "## Automatic Differentiation\n",
    "Automatic Differentiation (abbreviation: AD and also referred to as Algorithmic or Computational Differentiation) is a method to evaluate derivatives of real-valued functions. It is a variant of the classically conceptualized computer-based differentiation methods such as symbolic differentation and finite difference methods. It addresses shortcomings encountered in approaches such as symbolic and finite-difference differentiation. For example, symbolic differentation possesses the capability to compute derivatives to machine precision, however the required computational times can be quite large. On the other hand, finite difference differentiation is quicker but errors in the computed derivatives are several order of magnitudes higher than machine precision. AD emerges as a solution devoid of shortcomings in both symbolic and finite difference methods and hence is gaining popularity in computational scientific applications.\n",
    "\n",
    "The software package presented here provides a computational tool to compute derivatives of multivalued-functions employing the AD technique. The package will compute derivatives using the forward and reverse mode AD.\n",
    "\n",
    "## References\n",
    "1. Hoffmann, P. H. (2016). A hitchhikerâ€™s guide to automatic differentiation. Numerical Algorithms, 72(3), 775-811.\n",
    "2. van Merrienboer, B., Moldovan, D., & Wiltschko, A. (2018). Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming. In Advances in Neural Information Processing Systems (pp. 6256-6265).\n",
    "3. https://harvard-iacs.github.io/2020-CS107/lectures/.\n",
    "4. Griewank, A. and Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation (Vol. 105). Siam.\n",
    "5. Nocedal, J. and Wright, S., 2001. Numerical Optimization, Second Edition. Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory Structure\n",
    "Provided below is a proposed directory structure for the AD library.\n",
    "\n",
    "++--source_code  \n",
    "|   +-- ad_library.py  \n",
    "++--README  \n",
    "|   +-- README.md  \n",
    "++--Documentation  \n",
    "|   +-- doc_files  \n",
    "++--Set_up  \n",
    "|   +-- set_up_instructions_file\n",
    "\n",
    "\n",
    "## Python Modules to be used\n",
    "The Python modules that can be used are:\n",
    "- numpy for accessing user-input functions.\n",
    "- math for performing elementary mathematical operations.\n",
    "\n",
    "## Test Suite to be used\n",
    "The package will be tested using the TravisCI testing tool. The Github repository of this package has already been linked to TravisCI as part of milestone 1b.\n",
    "\n",
    "## Package Distribution\n",
    "The initial plan is to distribute the package on a Github repository with access available to clone and run it. The accompanying directories in the package will contain detailed instructions on how to run the package.\n",
    "\n",
    "## Framework for Software\n",
    "Currently there is no plan of using a framework. \n",
    "\n",
    "## Sample Source Code Structure of the Package\n",
    "Provided below is a very basic structure of the source code of this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_library():\n",
    "    \n",
    "    # Initialize the constructor for an AD object\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    # Function to parse the user input function\n",
    "    def parse_input(user_input):\n",
    "        pass\n",
    "    \n",
    "    # Function to implement forward automatic differentiation\n",
    "    def forward_ad(input_function):\n",
    "        pass\n",
    "    \n",
    "    # Function to implement backward automatic differentiation\n",
    "    def backward_ad(input_function):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "source": [
    "# How to Use AutoDiff\n",
    "\n",
    "1. The user will (save/download/install) the AutoDiff library. \n",
    "2. Once this is complete, they can import AutoDiff which will give them access to the ad_library class. \n",
    "3. The user will be able to construct an ad_library object in order to perform the available functions of our library. \n",
    "<br>\n",
    "Below we list some pseudocode to demonstrate general use cases.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ad_library\n",
    "\n",
    "my_function = (math.sin(x)) QUESTION HERE\n",
    "ad = adl()\n",
    "\n",
    "forward = ad.forward_ad(input_function, seed_vector = [1, 1])\n",
    "\n",
    "optimized = ad.optimization(parameters QUESTION HERE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Implementation\n",
    "\n",
    "## Core Data Structures:\n",
    "QUESTION HERE\n",
    "\n",
    "The \n",
    "\n",
    "## Classes:\n",
    "The primary class within this library will be the ad_library class which will contain the methods necessary to perform forward automatic differentiation, backward automatic differentiation, and optimization along with some helpers methods described below.\n",
    "### Method and name attributes:\n",
    "The ad_library object will contain the methods necessary to perform forward automatic differentiation and optimization. These will contained in their own methods with a few additional helper methods. These are listed below with descriptions.\n",
    "Thus, the primary method \n",
    "\n",
    "QUESTION: Name methods?\n",
    "\n",
    "## External Dependencies:\n",
    "- Numpy\n",
    "    - Numpy will be used in order to be build highly dynamic arrays. This will be necessary often throughout our implementation in order to...\n",
    "- Math\n",
    "    - The Math package will be used to deal with elementary functions and operations. This is necessary throughout differentiation in all modes.\n",
    "\n",
    "Dealing with elementary functions:\n",
    "These will all be implemented using the python math module which includes:\n",
    "- math.sin()\n",
    "- math.sqrt()\n",
    "- math.log()\n",
    "- math.exp()\n",
    "- An expansive list of elementary functions, numbers, and operations that can be accessed through the math module.\n",
    "\n",
    "Dealing with edge use cases:\n",
    "- Scalars and Vectors\n",
    "    - We will deal with vector inputs "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}