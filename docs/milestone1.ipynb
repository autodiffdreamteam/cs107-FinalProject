{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Importance of Differentiation \n",
    "**Differentiation** is one of the most commonly used mathematical operations. Fundamentally, it computes the rate of change of a variable with respect to another. This rate is referred to as the derivative of the first variable with respect to the second. Differentiation is also applied to single or multi-valued functions to compute their derivatives with respect to one or more dependent variables. Values of these derivatives serve as a critical input to several numerical algorithms (e.g. Newton's method) and considering the fast growing complexity of the computational problems being solved these days it is imperative to compute these derivatives accurately as well as quickly. \n",
    "\n",
    "## Automatic Differentiation\n",
    "**Automatic Differentiation (abbreviation: AD and also referred to as Algorithmic or Computational Differentiation)** is a method to evaluate derivatives of real-valued functions. It is a variant of the classically conceptualized computer-based differentiation methods such as symbolic differentation and finite difference methods. It addresses shortcomings encountered in approaches such as symbolic and finite-difference differentiation. For example, symbolic differentation possesses the capability to compute derivatives to machine precision, however the required computational times can be quite large. On the other hand, finite difference differentiation is quicker but errors in the computed derivatives are several order of magnitudes higher than machine precision. AD emerges as a solution devoid of shortcomings observed in both symbolic and finite difference methods and hence is gaining popularity in computational scientific applications.\n",
    "\n",
    "## Applications of Automatic Differentiation\n",
    "Automatic differentiation can be applied towards solving multiple important computational problems. Examples include root finding methods such as the Newton's method, optimization schemes such as the Gradient Descent method, and machine learning algorithms such as the backpropagation algorithm. All these methods have a critical dependence on quick computation of accurate derivates of the function being considered. Automatic differentiation serves these requirements and hence is an appropriate choice for calculating the derivatives for these algorithms.\n",
    "\n",
    "## The Gradient Descent Method \n",
    "The **Gradient Descent Method (GDM)** is an iterative optimization algorithm. It can be used to find the local minima of a differentiable function by iteratively miniminizing a cost function and ultimately stopping at the minimum value of the cost function which indicates a local minima.\n",
    "\n",
    "In 1-D the following steps are performed in GDM\n",
    "**Step 1**: Given a function in x, start at a random point x0\n",
    "\n",
    "**Step 2**: Compute the derivative at x0\n",
    "\n",
    "**Step 3**: Check if the algorithm is going in the right direction based on the sign of the derivative or have we reached the minima\n",
    "\n",
    "**Step 4**: Get a new value of x and repeat Steps 1-3 until converge to a local minima\n",
    "\n",
    "This approach can be extended to higher dimensions.\n",
    "\n",
    "## Scope of the Current Software Package\n",
    "The software package presented here provides a computational tool to compute derivatives of multivalued-functions employing the AD technique. The package will compute derivatives using the forward AD method. In addition, it would also include the capability of performing optimization using the GDM. \n",
    "\n",
    "\n",
    "## References\n",
    "1. Hoffmann, P. H. (2016). A hitchhikerâ€™s guide to automatic differentiation. Numerical Algorithms, 72(3), 775-811.\n",
    "2. van Merrienboer, B., Moldovan, D., & Wiltschko, A. (2018). Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming. In Advances in Neural Information Processing Systems (pp. 6256-6265).\n",
    "3. https://harvard-iacs.github.io/2020-CS107/lectures/.\n",
    "4. Griewank, A. and Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation (Vol. 105). Siam.\n",
    "5. Nocedal, J. and Wright, S., 2001. Numerical Optimization, Second Edition. Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory Structure\n",
    "Provided below is a proposed directory structure for the AD library.\n",
    "\n",
    "### Directory 1\n",
    "++--source_code \n",
    "|   +-- AutoDiffpy.py\n",
    "|   +-- GradDescpy.py\n",
    "|   +-- Testscript.py\n",
    "### Directory 2\n",
    "++--README  \n",
    "|   +-- README.md\n",
    "### Directory 3\n",
    "++--Documentation  \n",
    "|   +-- doc_files\n",
    "### Directory 4\n",
    "++--Set_up  \n",
    "|   +-- set_up_instructions_file\n",
    "\n",
    "\n",
    "## Python Modules to be used\n",
    "The Python modules that can be used are:\n",
    "- numpy for accessing user-input functions.\n",
    "- math for performing elementary mathematical operations.\n",
    "\n",
    "## Test Suite to be used\n",
    "The package will be tested using the TravisCI testing tool. The Github repository of this package has already been linked to TravisCI as part of milestone 1b.\n",
    "\n",
    "## Package Distribution\n",
    "The initial plan is to distribute the package on a Github repository with access available to clone and run it. The accompanying directories in the package will contain detailed instructions on how to run the package.\n",
    "\n",
    "## Framework for Software\n",
    "Currently there is no plan of using a framework. \n",
    "\n",
    "## Sample Structure of the Package\n",
    "Figure 1 shows a use case diagram for the software package. The actors in the diagram include the user and the two classes (AutoDiffpy and GradDespy) to perform the autodifferentiation and optimization respectively. The user can use the package to just differentiate the function or to find the optimum value of a function using GDM.  \n",
    "\n",
    "![Use_Case_Diagram](UseCaseDiagram0.png)\n",
    "**Figure 1: Use case diagram for the package**\n",
    "\n",
    "Figure 2 shows a class diagram for the package. The package consists of only two classes **(AutoDiffpy and GradDespy)** which contain methods to perform the autodifferentiation and optimization respectively. There is also a testscript which acts as an API. It will read the user input, create the appropriate object (AD or GDM), and use the properties of the object (methods and attributes) to perform the required tasks (i.e AD or optimization using AD).The current implementation includes python lists as the only data structures used for storage. However, this is preliminary and can be changed once development begins.\n",
    "\n",
    "![Class_Diagram](ClassDiagram0.png)\n",
    "**Figure 2: Class diagram for the package**\n",
    "\n",
    "Provided below is a very basic structure of the source code of this package. This structure is still tentative and can be revised based on further feedback and discussion.\n",
    "\n",
    "## Class AutoDiffpy\n",
    "This is the class to perform AD forward mode. It contains a method to parse the user input function and get which basic functions exist (e.g. sine, cosine) and how many variables there are (x, (x,y), (x,y,z)). The AD object would have properties such as Jacobian etc. associated with it. The method get_Jacobian could be used to compute the Jacobian matrix while the method get_derivative could be used to compute the required derivative.\n",
    "\n",
    "## Class GradDescpy\n",
    "This is the class to perform the GDM. It also contains a method to parse the user input function. It also has methods to compute the objective function (compute_ObjFunc) and get the function values at different iterations (get_funcValues). These would again be properties associated with the GD object and can be provided to the user once the computation is done.\n",
    "\n",
    "## Class Testscript\n",
    "This is the API class which prompts the user for input and then calls the appropriate classes (AD or GradDesc) to perform the required tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoDiffpy():\n",
    "    \n",
    "    # Initialize the constructor for an AD object\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    # Function to parse the user input function\n",
    "    def parse_input(user_input):\n",
    "        pass\n",
    "    \n",
    "    # Function to implement forward automatic differentiation\n",
    "    def get_Jacobian():\n",
    "        pass\n",
    "    \n",
    "    def get_derivative():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "def GradDescpy():\n",
    "    \n",
    "    # Initialize the constructor for a GDM object\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def parse_input(user_input):\n",
    "        pass\n",
    "    \n",
    "    def compute_ObjFunc():\n",
    "        pass\n",
    "    \n",
    "    def get_funcValues():\n",
    "        pass\n",
    "    \n",
    "\n",
    "def Testscript():\n",
    "    \n",
    "    #get user input and perform the task needed (AD or GradDesc)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
