{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Importance of Differentiation \n",
    "Differentiation is one of the most commonly used mathematical operations. Fundamentally, it computes the rate of change of a variable with respect to another. This rate is referred to as the derivative of the first variable with respect to the second. Differentiation is also applied to single or multi-valued functions to compute their derivatives with respect to one or more dependent variables. Values of these derivatives serve as a critical input to several numerical algorithms (e.g. Newton's method) and considering the fast growing complexity of the computational problems being solved these days it is imperative to compute these derivatives accurately as well as quickly. \n",
    "\n",
    "## Automatic Differentiation\n",
    "Automatic Differentiation (abbreviation: AD and also referred to as Algorithmic or Computational Differentiation) is a method to evaluate derivatives of real-valued functions. It is a variant of the classically conceptualized computer-based differentiation methods such as symbolic differentation and finite difference methods. It addresses shortcomings encountered in approaches such as symbolic and finite-difference differentiation. For example, symbolic differentation possesses the capability to compute derivatives to machine precision, however the required computational times can be quite large. On the other hand, finite difference differentiation is quicker but errors in the computed derivatives are several order of magnitudes higher than machine precision. AD emerges as a solution devoid of shortcomings in both symbolic and finite difference methods and hence is gaining popularity in computational scientific applications.\n",
    "\n",
    "## Applications of Automatic Differentiation\n",
    "Automatic differentiation can be applied towards solving multiple important computational problems. Examples include root finding methods such as the Newton's method, optimization schemes such as the Gradient Descent method, and machine learning algorithms such as the backpropagation algorithm. All these methods have a critical dependence on computing accurate derivates of the function being considered in quick time. Automatic differentiation serves these requirements and hence is an appropriate choice for calculating the derivatives for these algorithms.\n",
    "\n",
    "## The Gradient Descent Method \n",
    "The Gradient Descent Method (GDM) is an iterative optimization algorithm. It can be used to find the local minima of a differentiable function by iteratively miniminizing a cost function and ultimately stopping at the minimum value of the cost function which indicates a local minima.  \n",
    "\n",
    "## Scope of the Current Software Package\n",
    "The software package presented here provides a computational tool to compute derivatives of multivalued-functions employing the AD technique. The package will compute derivatives using the forward AD method. In addition, it would also include the capability of performing optimization using the GDM. \n",
    "\n",
    "\n",
    "## References\n",
    "1. Hoffmann, P. H. (2016). A hitchhikerâ€™s guide to automatic differentiation. Numerical Algorithms, 72(3), 775-811.\n",
    "2. van Merrienboer, B., Moldovan, D., & Wiltschko, A. (2018). Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming. In Advances in Neural Information Processing Systems (pp. 6256-6265).\n",
    "3. https://harvard-iacs.github.io/2020-CS107/lectures/.\n",
    "4. Griewank, A. and Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation (Vol. 105). Siam.\n",
    "5. Nocedal, J. and Wright, S., 2001. Numerical Optimization, Second Edition. Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory Structure\n",
    "Provided below is a proposed directory structure for the AD library.\n",
    "\n",
    "++--source_code  \n",
    "|   +-- ad_library.py  \n",
    "++--README  \n",
    "|   +-- README.md  \n",
    "++--Documentation  \n",
    "|   +-- doc_files  \n",
    "++--Set_up  \n",
    "|   +-- set_up_instructions_file\n",
    "\n",
    "\n",
    "## Python Modules to be used\n",
    "The Python modules that can be used are:\n",
    "- numpy for accessing user-input functions.\n",
    "- math for performing elementary mathematical operations.\n",
    "\n",
    "## Test Suite to be used\n",
    "The package will be tested using the TravisCI testing tool. The Github repository of this package has already been linked to TravisCI as part of milestone 1b.\n",
    "\n",
    "## Package Distribution\n",
    "The initial plan is to distribute the package on a Github repository with access available to clone and run it. The accompanying directories in the package will contain detailed instructions on how to run the package.\n",
    "\n",
    "## Framework for Software\n",
    "Currently there is no plan of using a framework. \n",
    "\n",
    "## Sample Structure of the Package\n",
    "Figure 1 shows a use case diagram for the software package. The actors in the diagram include the user and the two classes (AutoDiffpy and GradDespy) to perform the autodifferentiation and optimization respectively. The user can use the package to just differentiate the function or to find the optimum value of a function using GDM.  \n",
    "\n",
    "![Use_Case_Diagram](UseCaseDiagram0.png)\n",
    "Figure 1: Use case diagram for the package\n",
    "\n",
    "Figure 2 shows a class diagram for the package. The package consists of only two classes (AutoDiffpy and GradDespy) to perform the autodifferentiation and optimization respectively. There is also a testscript which acts as an API, will read the user input, create the appropriate object (AD or GDM), and use the properties of the object to perform the required tasks (i.e AD or optimization using AD).\n",
    "\n",
    "![Class_Diagram](ClassDiagram0.png)\n",
    "\n",
    "Provided below is a very basic structure of the source code of this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoDiffpy():\n",
    "    \n",
    "    # Initialize the constructor for an AD object\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    # Function to parse the user input function\n",
    "    def parse_input(user_input):\n",
    "        pass\n",
    "    \n",
    "    # Function to implement forward automatic differentiation\n",
    "    def get_Jacobian():\n",
    "        pass\n",
    "    \n",
    "    def get_derivative():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "def GradDescpy():\n",
    "    \n",
    "    # Initialize the constructor for a GDM object\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def parse_input(user_input):\n",
    "        pass\n",
    "    \n",
    "    def compute_ObjFunc():\n",
    "        pass\n",
    "    \n",
    "    def get_funcValues():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
