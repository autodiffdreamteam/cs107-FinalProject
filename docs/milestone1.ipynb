{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Importance of Differentiation \n",
    "Differentiation is one of the most commonly used mathematical operations. Fundamentally, it computes the rate of change of a variable with respect to another. This rate is referred to as the derivative of the first variable with respect to the second. Differentiation is also applied to single or multi-valued functions to compute their derivatives with respect to one or more dependent variables. Values of these derivatives serve as a critical input to several numerical algorithms (e.g. Newton's method) and considering the fast growing complexity of the computational problems being solved these days it is imperative to compute these derivatives accurately as well as quickly. \n",
    "\n",
    "## Automatic Differentiation\n",
    "Automatic Differentiation (abbreviation: AD and also referred to as Algorithmic or Computational Differentiation) is a method to evaluate derivatives of real-valued functions. It is a variant of the classically conceptualized computer-based differentiation methods such as symbolic differentation and finite difference methods. It addresses shortcomings encountered in approaches such as symbolic and finite-difference differentiation. For example, symbolic differentation possesses the capability to compute derivatives to machine precision, however the required computational times can be quite large. On the other hand, finite difference differentiation is quicker but errors in the computed derivatives are several order of magnitudes higher than machine precision. AD emerges as a solution devoid of shortcomings in both symbolic and finite difference methods and hence is gaining popularity in computational scientific applications.\n",
    "\n",
    "The software package presented here provides a computational tool to compute derivatives of multivalued-functions employing the AD technique. The package will compute derivatives using the forward and reverse mode AD.\n",
    "\n",
    "## References\n",
    "1. Hoffmann, P. H. (2016). A hitchhikerâ€™s guide to automatic differentiation. Numerical Algorithms, 72(3), 775-811.\n",
    "2. van Merrienboer, B., Moldovan, D., & Wiltschko, A. (2018). Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming. In Advances in Neural Information Processing Systems (pp. 6256-6265).\n",
    "3. https://harvard-iacs.github.io/2020-CS107/lectures/.\n",
    "4. Griewank, A. and Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation (Vol. 105). Siam.\n",
    "5. Nocedal, J. and Wright, S., 2001. Numerical Optimization, Second Edition. Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory Structure\n",
    "Provided below is a proposed directory structure for the AD library.\n",
    "\n",
    "++--source_code  \n",
    "|   +-- ad_library.py  \n",
    "++--README  \n",
    "|   +-- README.md  \n",
    "++--Documentation  \n",
    "|   +-- doc_files  \n",
    "++--Set_up  \n",
    "|   +-- set_up_instructions_file\n",
    "\n",
    "\n",
    "## Python Modules to be used\n",
    "The Python modules that can be used are:\n",
    "- numpy for accessing user-input functions.\n",
    "- math for performing elementary mathematical operations.\n",
    "\n",
    "## Test Suite to be used\n",
    "The package will be tested using the TravisCI testing tool. The Github repository of this package has already been linked to TravisCI as part of milestone 1b.\n",
    "\n",
    "## Package Distribution\n",
    "The initial plan is to distribute the package on a Github repository with access available to clone and run it. The accompanying directories in the package will contain detailed instructions on how to run the package.\n",
    "\n",
    "## Framework for Software\n",
    "Currently there is no plan of using a framework. \n",
    "\n",
    "## Sample Source Code Structure of the Package\n",
    "Provided below is a very basic structure of the source code of this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ad_forward:\n",
    "    \n",
    "    # Initialize the constructor for an AD object\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    # Function to parse the user input function\n",
    "    def parse_input(user_input):\n",
    "        pass\n",
    "    \n",
    "    # Function to implement forward automatic differentiation\n",
    "    def forward_ad(input_function):\n",
    "        pass\n",
    "class ad_optimizer:\n",
    "\n",
    "    def __init__(parameters, cost_function, optimization_function):\n",
    "        params = parameters\n",
    "        cost_function = cost_function\n",
    "        optimization_function = optimization_function\n",
    "\n",
    "    # Function to implement backward automatic differentiation\n",
    "    def optimizer(self.params, self.cost_function, self.optimization_function):\n",
    "        pass\n",
    "        \n",
    "    def set_params(parameters):\n",
    "        params = parameters\n",
    "    \n",
    "    def get_params():\n",
    "        return self.params\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "# How to Use ad_library\n",
    "\n",
    "1. The user will install the ad_library library. \n",
    "2. Once this is complete, they can import the module ad_library which will give them access to the ad_forward and ad_optimizer classes. \n",
    "3a. The user will be able to construct an ad_library object in order to perform forward automatic differentation operations on a given function.\n",
    "3b. The user will be able to construct an ad_optimizer class in order to perform optimization using a gradient descent algorithm.\n",
    "<br>\n",
    "Below we list some pseudocode to demonstrate general use cases.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ad_library\n",
    "\n",
    "# Part 1: Forward Automatic Differentation\n",
    "\n",
    "# User defines function on which to run forward automatic differentation\n",
    "my_function = sample_function\n",
    "\n",
    "# Construct ad_forward class\n",
    "ad_for = ad_forward()\n",
    "\n",
    "# Run forward automatic differentation on a sample function\n",
    "user_forward = ad_for.forward_ad(input_function, seed_vector = [1, 1])\n",
    "\n",
    "# Part 2: Optimization via Gradient Descent\n",
    "params = sample_params\n",
    "cost_function_value = sample_cfv\n",
    "optimization_function = sample_of\n",
    "vector = sample_vector\n",
    "\n",
    "# Construct ad_optimizer class\n",
    "ad_opt = ad_optimizer(params, cost_function_value, optimization_function)\n",
    "\n",
    "#\n",
    "optimized = ad.optimization(vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Implementation\n",
    "\n",
    "## Core Data Structures:\n",
    "Arrays: These will be necessary throughout optimization and forward automatic differentation. We will be using numpy arrays to represent vectors.\n",
    "\n",
    "\n",
    "## Classes:\n",
    "The two primary classes within this library will be the ad_forward class which will contain the methods necessary to perform forward automatic differentiation, while the ad_optimizer class will be used to perform optimization via the gradient descent algorithm. Their methods and attributes are described below.\n",
    "\n",
    "### Method and name attributes:\n",
    "The forward_ad object will contain the methods necessary to perform forward automatic differentiation and optimization. These will contained in their own methods with a few additional helper methods. These are listed below with descriptions.\n",
    "\n",
    "#### ad_forward\n",
    "- __init__()\n",
    "    - Parameters: 0 Parameters\n",
    "    - Returns: A constructed ad_library object\n",
    "- parse_function(user_input)\n",
    "    - Parameters: A raw user input representing a function\n",
    "    - Returns: The parsed function to input into forward_ad \n",
    "- forward_ad(input_function, seed_vector = default)\n",
    "    - Parameters: An input function and an optional seed vector parameter.\n",
    "    - Returns: The result of forward automatic differentation on the given function.\n",
    "\n",
    "#### ad_optimizer\n",
    "Attributes:\n",
    "- params\n",
    "     - params contains vector of coefficients\n",
    "- optimization_function\n",
    "    - The optimization function is the optimization function for gradient descent\n",
    "- cost_function_value\n",
    "    - This contains the value of the cost function\n",
    "\n",
    "Methods:\n",
    "- __init__(parameters, cost_function_value = default_cfv, optimization_function = default_of)\n",
    "    - Parameters: Parameters which include an array representing the vector, and optional parameters which define the cost function value and the optimization function\n",
    "    - Returns: A constructed ad_optimizer object to perform optimization via gradient descent.\n",
    "- parse_function(user_input)\n",
    "    - Parameters: Raw user input representing a function.\n",
    "    - Returns: The parsed function to input into the optimize function \n",
    "- optimize(vector)\n",
    "    - Performs gradient descent given a cost function and a set of vectors\n",
    "\n",
    "\n",
    "## External Dependencies:\n",
    "- Numpy\n",
    "    - Numpy will be used in order to be build highly dynamic arrays. This will be necessary often throughout our implementation in order to...\n",
    "- Math\n",
    "    - The Math package will be used to deal with elementary functions and operations. This is necessary throughout differentiation in all modes.\n",
    "\n",
    "Dealing with elementary functions:\n",
    "These will all be implemented using the python math module which includes:\n",
    "- math.sin()\n",
    "- math.sqrt()\n",
    "- math.log()\n",
    "- math.exp()\n",
    "- An expansive list of elementary functions, numbers, and operations that can be accessed through the math module.\n",
    "\n",
    "Dealing with edge use cases:\n",
    "- Scalars and Vectors\n",
    "    - We will deal with vector inputs using .... FINISH"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}