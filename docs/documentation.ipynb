{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Differentiation \n",
    "Differentiation is one of the most commonly used mathematical operations in computational sciences. Fundamentally, it computes the rate of change of a variable with respect to another. This rate is referred to as the derivative of the first variable with respect to the second. Differentiation generalizes from scalar to vector-valued functions, for which we can compute derivatives with respect to one or more dependent variables. Values of these derivatives serve as critical inputs to many root-finding methods (e.g. Newton's method), optimization algorithms (e.g. gradient descent), and interpolation methods (e.g. quadratic splines). Given the rapid growth in the complexity of modern computational problems, it is imperative to compute these derivatives both quickly and accurately. \n",
    "\n",
    "\n",
    "## Automatic differentiation (AD)\n",
    "Automatic differentiation (abbreviation: AD and also referred to as Algorithmic or Computational Differentiation) is a method to evaluate derivatives of real-valued functions. It is a variant of the classically conceptualized computer-based differentiation methods such as symbolic differentation and finite difference methods but addresses shortcomings encountered in both. In particular, symbolic differentation can compute derivatives to machine precision, but requires a high computational load for complicated functions. Finite difference differentiation, on the other hand, is faster but produces errors in the computed derivatives that are several order of magnitudes higher than machine precision. AD emerges as a solution that addresses both of these shortcomings and hence is gaining popularity in computational scientific applications.\n",
    "\n",
    "\n",
    "## Applications of automatic differentiation\n",
    "AD can be applied to many critical computational problems. Examples include root-finding methods such as Newton's method, optimization schemes such as gradient descent (and its variants, including stochastic gradient descent and Nesterov's accelerated gradient descent), and interpolation methods such as quadratic splines. As modern scientific research uses increasingly large datasets, all these methods rely on fast and accurate computation of derivates. Automatic differentiation serves these requirements and hence is an appropriate choice for calculating the derivatives for these algorithms.\n",
    "\n",
    "\n",
    "## Scope\n",
    "The `DreamDiff` Python package provides an efficient and easy-to-use computational tool for computing the derivatives of scalar and vector-valued functions employing forward mode AD. In addition, `DreamDiff` uses its core AD functionality to implement root-finding, optimization, and interpolation methods, including Newton's method, gradient descent, Nesterov's accelerated gradient descent, and quadratic splines. The `Optimize` class within `DreamOptimize` also enables users to easily visualize these optimization methods by creating `matplotlib` animations. To provide some additional context for the functionality utilized by `Optimize`, an explanation of Newton's method is provided below (see the **Extension** section for a full details on the other methods implemented in this package).\n",
    "\n",
    "\n",
    "## Newton's method\n",
    "Newton's method provides an iterative approach to finding the roots of differentiable functions.\n",
    "\n",
    "In one-dimension, Newton's method performs the following steps:\n",
    "\n",
    "**Step 1**: Given a function $f$, start at a random point $x_0$\n",
    "\n",
    "**Step 2**: Compute $f(x_0)$ and $f'(x_0)$\n",
    "\n",
    "**Step 3**: Set $x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$\n",
    "\n",
    "**Step 4**: If $|f(x_1)| < \\epsilon$, where $\\epsilon$ is a small tolerance parameter, then stop. \n",
    "            Otherwise, set $x_0 = x_1$\n",
    "\n",
    "**Step 5**: Repeat steps 2-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## The chain rule\n",
    "\n",
    "Automatic differentitation is built upon the chain rule, which provides a formula to compute the derivative of composite functions. Recall that if we have a composite function\n",
    "\n",
    "$$ f(g(h(x))), $$\n",
    "\n",
    "the chain rule tells us that\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial h}\\frac{\\partial h}{\\partial t}. $$\n",
    "\n",
    "In this package, we implement the forward mode of AD, whereby we evaluate the derivative of a composite function from the inside out. Put differently, we differentiate with respect to one independent variable and recursively find the derivative of each sub-expression starting with the innermost function. In general, the forward mode would evaluate the derivative above as follows:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial v_{n-1}} \\frac{\\partial v_{n-1}}{\\partial x} = \\frac{\\partial f}{\\partial v_{n-1}}  \\left( \\frac{\\partial v_{n-1}}{\\partial v_{n-2}}\\frac{\\partial v_{n-2}}{\\partial x} \\right) = \\frac{\\partial f}{\\partial v_{n-1}} \\frac{\\partial v_{n-1}}{\\partial x} = \\frac{\\partial f}{\\partial v_{n-1}}  \\left( \\frac{\\partial v_{n-1}}{\\partial v_{n-2}} \\left( \\frac{\\partial v_{n-2}}{\\partial v_{n-3}} \\frac{\\partial v_{n-3}}{\\partial x} \\right) \\right) = ..., $$\n",
    "\n",
    "where each $ v_i $ is an inner function. If the function we want to differentiate has multiple arguments, as in\n",
    "\n",
    "$$ f(g(t), h(t)), $$\n",
    "\n",
    "then the chain rule extends naturally, giving\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x} + \\frac{\\partial f}{\\partial h}\\frac{\\partial h}{\\partial x}. $$\n",
    "\n",
    "And if we have a vector-valued function with $ x \\in \\mathbb{R}^m $\n",
    "\n",
    "$$ f = f(g(x), h(x)), $$\n",
    "\n",
    "the derivative becomes\n",
    "\n",
    "$$ \\nabla_x f = \\frac{\\partial f}{\\partial g} \\nabla g + \\frac{\\partial f}{\\partial h} \\nabla h. $$\n",
    "\n",
    "Putting this all together, we can use the chain rule to express the derivative of a general vector-valued function $ f=f(y(x)) $ where $ y \\in \\mathbb{R}^n $ and $ x \\in \\mathbb{R}^m $ as\n",
    "\n",
    "$$ \\nabla_x f = \\sum_{i=1}^n \\frac{\\partial f}{\\partial y_i} \\nabla y_i(x). $$\n",
    "\n",
    "\n",
    "## The computational graph\n",
    "\n",
    "Consider the function $ f(x, y) = e^{x^2 + y^2}. $ For illustrative purposes, we can summarize the recursive operations of forward mode AD in a computational graph. \n",
    "\n",
    "Let's say we want to find $ \\partial f / \\partial x. $ Because we are differentiating with respect to $ x, $ we start by calculating seed values as:\n",
    "\n",
    "$$ \\frac{\\partial x}{\\partial x} = 1, $$ and\n",
    "$$ \\frac{\\partial y}{\\partial x} = 0. $$\n",
    "\n",
    "Then the derivative is evaluated as follows:\n",
    "\n",
    "| Trace   | Elementary Operation   | Derivative Operation                     |\n",
    "|---------|------------------------|------------------------------------------|\n",
    "| $v_1$   | $x$                    | $\\dot{v_1}=1$ (seed)                     |\n",
    "| $v_2$   | $v_1^2$                | $\\dot{v_2}=2v_1 \\dot{v_1}$               |\n",
    "| $v_3$   | $y$                    | $\\dot{v_3}=0$ (seed)                     |\n",
    "| $v_4$   | $v_3^2$                | $\\dot{v_4}=2v_3 \\dot{v_3}$               |\n",
    "| $v_5$   | $v_2 + v_4$            | $\\dot{v_5}=\\dot{v_2} + \\dot{v_4}$        |\n",
    "| $f$     | $e^{v_5}$              | $\\partial f/\\partial x=\\dot{v_5}e^{v_5}$ |\n",
    "\n",
    "In essence, forward mode AD does computationally what we did by hand in the table above. In general, it computes the product $\\nabla f \\cdot p,$ where $ p $ is a seed vector computed as we did in the example above. If $ f $ is a vector-valued function, then it computes $ Jp, $ where $ J $ is the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use\n",
    "\n",
    "`DreamDiff` is installable via PyPI or available for download on GitHub.\n",
    "\n",
    "\n",
    "## Installation via PyPI\n",
    "\n",
    "# NOT DONE YET\n",
    "\n",
    "\n",
    "## Download on GitHub\n",
    "1. Running this application requires the Python language. Please download it from: https://www.python.org/downloads/ depending on the operating system.\n",
    "2. Run the installation process and set up the Python Path correctly on your system so that you can access it from the terminal (Windows command prompt or bash shell in MAC OS).\n",
    "3. Type `python` on your system shell (Windows command prompt or bash shell in MAC OS) to start the Python runtime environment or you can use any of the graphical user interfaces such as Visual Studio, or Python IDLE to start running Python.\n",
    "4. `git clone https://github.com/autodiffdreamteam/cs107-FinalProject.git` and make sure the AutoDiff.py class is in your working directory.\n",
    "5. Create your Python script to run.\n",
    "6. Install the dependencies in `requirements.txt` including `NumPy` and `pytest` (if running testing tasks).\n",
    "7. Run the program. Instructions on how to do this are provided below.\n",
    "\n",
    "\n",
    "## `DreamDiff` Demo\n",
    "After installing `DreamDiff`, you are ready to being utilizing three core classes: `DreamDiff`, which performs forward mode AD on a function and stores its value and derivative, `Function`, which allows you to define more complex functions including trig, inverse trig, hyperbolic, exponential, logarithmic, square root, and logistic functions, and `Optimize`, which provides tools for root-finding, optimization, and interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\DreamDiff\")\n",
    "\n",
    "from DreamDiff import DreamDiff\n",
    "from DreamDiff import Function\n",
    "from DreamOptimize import Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a scalar function evaluated at the point `2.0` is simple. Note that for the simple scalar case, the derivative is stored in a 1x1 Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "[2.]\n",
      "Jacobian:\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "x = DreamDiff(2.0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After created a `DreamDiff` object, you can easily increase the complexity of your function using standard mathematical operations like addition, subtraction, multiplication, division, and power. For example, you can find the value of $f(x) = \\frac{1}{2}(x^2 + 5x - 3)$ and its derivative at $x = 2$ by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "[5.5]\n",
      "Jacobian:\n",
      "[4.5]\n"
     ]
    }
   ],
   "source": [
    "f = (x**2 + 5*x - 3) / 2\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the suite of functions provided in `Function`, you can make your functions much more complex. For example, you could define $f(x) = e^{\\sin(x)} - \\cos(\\sqrt{x}) \\sin(\\sqrt{\\cos^2(x) + x^2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "[2.3436876]\n",
      "Jacobian:\n",
      "[-0.6395236]\n"
     ]
    }
   ],
   "source": [
    "f = Function.exp(Function.sin(x)) - Function.cos(x**0.5)*Function.sin((Function.cos(x)**2 + x**2)**0.5)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DreamDiff` also allows you to pass multiple inputs into a function using either manually-entered or default seed vectors. For example, if you wanted to evaluate and differentiate $f(x, y) = 2x + y$ at $x=2, y=3,$ you can create vector inputs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "[7.]\n",
      "Jacobian:\n",
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "x = DreamDiff(2.0, [1, 0])\n",
    "y = DreamDiff(3.0, [0, 1])\n",
    "f = 2*x + y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high-dimensional problems, `DreamDiff` provides the option to easily scale vector inputs with the optional parameter `input_pos`, which accepts a two-element list, the first element of which is the total number of input variables, and the second indicates the index of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "[-4.6]\n",
      "Jacobian:\n",
      "[ 0.2 -3.   1. ]\n"
     ]
    }
   ],
   "source": [
    "x = DreamDiff(2.0, input_pos=[3, 0])\n",
    "y = DreamDiff(3.0, input_pos=[3, 1])\n",
    "z = DreamDiff(4.0, input_pos=[3, 2])\n",
    "f = x/5 - y*3 + z\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `DreamDiff` also supports vector inputs with vector functions, which you can define by passing multiple vector inputs in a list into a new `DreamDiff` object. With two inputs and two functions, we obtain a 2x2 Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "[[8.]\n",
      " [9.]]\n",
      "Jacobian:\n",
      "[[4. 2.]\n",
      " [0. 6.]]\n"
     ]
    }
   ],
   "source": [
    "x = DreamDiff(2.0, [1, 0])\n",
    "y = DreamDiff(3.0, [0, 1])\n",
    "f1 = x + y*x\n",
    "f2 = y**2\n",
    "f = DreamDiff([f1, f2])\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Optimize` Demo\n",
    "`Optimize` implements Newton's method for finding roots of scalar functions. In order to achieve this, two private methods in the `DreamDiff` class are used to handle the parsing and evaluate a function entered as a string. After parsing this user-defined string, `DreamDiff` uses its core AD implementation compute derivatives needed for Newton's method.\n",
    "\n",
    "For example, let $f(x) = x^3-3x^2+4$. By passing a string\n",
    "representation of the function to Newton's method along with an initial evaluation point, an epsilon threshold,\n",
    "and the maximum number of iterations for which to run the algorithm, the `newtons_method` function will return a\n",
    "tuple containing the root, a list containing the previous points at which $f$ was evaluated, a list of $f(x),$ \n",
    "values and a list of $f'(x)$ values, or `None` if no root was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solution after 10 iterations.\n",
      "Solution is: 2.0018221208716365\n"
     ]
    }
   ],
   "source": [
    "f = 'x^3 - 3*x^2 + 4'\n",
    "results = Optimize.newtons_method(f, 0.3, epsilon=0.00001, max_iters=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optimize` also provides a method that creates a `matplotlib` animation showing how the root was located using Newton's\n",
    "method. Running the commented line below will plot the animation, from which we have included a single frame for reference. Please see the **Extension** section below for more details on `Optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'x^3 - 3*x^2 + 4'\n",
    "#Optimize.animate_newtons(f, 0.3, epsilon=0.000001, max_iters=500, runtime=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![newtons_animate.png](newtons_animate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "\n",
    "## Directory structure\n",
    "Our GitHub directory is organized as follows:\n",
    "\n",
    "```\n",
    "cs107-FinalProject/\n",
    "        README.md\n",
    "        requirements.txt\n",
    "        docs/\n",
    "            documentation.ipynb\n",
    "            milestone1.ipynb\n",
    "            milestone2.ipynb\n",
    "            milestone2_progress.md\n",
    "        DreamDiff/\n",
    "            DreamDiff.py\n",
    "            DreamOptimize.py\n",
    "        tests/\n",
    "            test_DreamDiff.py\n",
    "            test_DreamOptimize.py\n",
    "        demo/\n",
    "            dreamdiffdemo.ipynb\n",
    "```\n",
    "\n",
    "\n",
    "## Python modules\n",
    "See `requirements.txt` for a list of dependencies. These include:\n",
    "- `NumPy` for performing elementary mathematical operations in `AutoDiff`.\n",
    "- `matplotlib` for creating visualizations and animations in `DreamOptimize`.\n",
    "- `pytest` for testing.\n",
    "\n",
    "\n",
    "## Test suite\n",
    "All the software in this package is tested using TravisCI. Tests for `AutoDiff` are found in `test_AutoDiff.py`, and tests for `Optimize` are found in `test_DreamOptimize.py`. They are run automatically and used to generate our Codecov badge as shown in `README.md`.\n",
    "\n",
    "\n",
    "## Package distribution\n",
    "Our package is installable via `PyPI` and available for download from GitHub. See the **How to use** section above for details.\n",
    "\n",
    "\n",
    "# Implementation\n",
    "\n",
    "\n",
    "## Core classes\n",
    "\n",
    "The functionality within this package is built upon two core classes: `DreamDiff` and `Optimize`, which are located in the files `DreamDiff.py` and `DreamOptimize.py`, respectively. `DreamDiff` enables uses to construct scalar or vector-valued functions with scalar or vector inputs upon which to perform forward-mode automatic differentiation (see the **`DreamDiff` Demo** above for details.)\n",
    "\n",
    "The `Optimize` class builds upon the core functionality in `DreamDiff`, providing tools to find the roots of scalar functions using Newton's method, minimize scalar functions using gradient descent and Nesterov's accelerated gradient descent, and finally interpolate using quadratic splines. `Optimize` also provides methods to visualize each of these, and they are elaborated in the **Extension** section below.\n",
    "\n",
    "\n",
    "## Data structures\n",
    "\n",
    "`DreamDiff` and `DreamOptimize` make use of a broad range of data structures, including: \n",
    "- 1-D and 2-D `NumPy` arrays for optimized storage and fast retrieval of data (e.g. derivatives in the Jacobian)\n",
    "- Tuples to conveniently store pairs of values, for example `(f.val, f.der)` \n",
    "- Python lists, for example to store the history of derivatives in the Newton's method and gradient descent algorithms.\n",
    "\n",
    "\n",
    "## External dependencies\n",
    "\n",
    "See `requirements.txt`\n",
    "\n",
    "\n",
    "## Elementary functions\n",
    "\n",
    "In order to allow users to construct and compute the derivatives of complex functions, the `AutoDiffPy` class overloads all the basic mathematical operations, including:\n",
    "```\n",
    "__add__ # addition\n",
    "__radd__ # right-side addition\n",
    "__sub__ # subtraction\n",
    "__rsub__ # right-side subtraction\n",
    "__mul__ # multiplication\n",
    "__rmul__ # right-side multiplication\n",
    "__truediv__ # division\n",
    "__rtruediv__ # right-side division\n",
    "__pow__ # power\n",
    "__rpow__ # right-side power\n",
    "__neg__ # negation\n",
    "__pos__ # + operator\n",
    "__abs__ # absolute value\n",
    "```\n",
    "\n",
    "In addition, the following comparison operators are overloaded:\n",
    "```\n",
    "__eq__ # equal to\n",
    "__ne__ # not equal\n",
    "__lt__ # less than\n",
    "__gt__ # greater than\n",
    "__le__ # less than or equal to\n",
    "__ge__ # greater than or equal to\n",
    "```\n",
    "\n",
    "\n",
    "Finally, the `Function` class within `DreamDiff.py` includes common elementary functions, including:\n",
    "```\n",
    "Function.sin(x)\n",
    "Function.cos(x)\n",
    "Function.tan(x)\n",
    "Function.arcsin(x)\n",
    "Function.arccos(x)\n",
    "Function.arctan(x)\n",
    "Function.sinh(x)\n",
    "Function.cosh(x)\n",
    "Function.tanh(x)\n",
    "Function.sqrt(x)\n",
    "Function.exp(x)\n",
    "Function.log(x)\n",
    "Function.log2(x)\n",
    "Function.log10(x)\n",
    "Function.logistic(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension\n",
    "\n",
    "## Root-Finding\n",
    "\n",
    "`DreamDiff` implements Newton's method for finding roots of scalar functions. In order to achieve this, we\n",
    "implemented two private methods in the `DreamDiff` class to handle the parsing and evaluation of a function\n",
    "entered as a string. After parsing this user-defined string, `DreamDiff` uses its core AD implementation compute \n",
    "derivatives needed for Newton's method, which is accessed via the `Optimize` class.\n",
    "\n",
    "See the `Optimize` demo for an example that uses Newton's method for root-finding.\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Beyond root-finding, `DreamDiff` is also equipped with a set of optimization algorithms to find local minima\n",
    "of functions. It includes an implementation of the standard gradient descent method, as well as an optimized \n",
    "version called Nesterov's accelerated gradient descent, which relies on an additional momentum parameter.\n",
    "\n",
    "**Standard gradient descent:**\n",
    "\n",
    "1. Inputs received by the function (expected: function (`f`, str), starting point (`x0`, int, float),  convergence threshold (`epsilon`, int, float), maximum number of iterations to be done (`max_iters`, int), learning rate (`eta`, int, float)\n",
    "2. Store the starting point in a variable `xn`\n",
    "3. Create a `DreamDiff` object \n",
    "4. Parse the input function by calling `_parse_input`\n",
    "5. Create a list `xn_history` to store all the points that the algorithm encounters while moving towards the minimum\n",
    "6. Start looping within the range set by the user (`max_iters`)\n",
    "7. (Within the loop) Add the starting point to `xn_history`\n",
    "8. (Within the loop) Get the value and derivative at `xn`\n",
    "9. (Within the loop) Compute the new value of `xn` using the current value, derivative, and learning rate\n",
    "10. (Within the loop) Check for convergence by computing if the difference between new value of `xn` from Step 9 and current value is less than `epsilon`. If convergence achieved return the value of `xn` and stop\n",
    "11. (Within the loop) If convergence not reached continue the next iteration (Step 6) with new value of `xn`\n",
    "12. If end of the loop is reached without convergence then print out message indicating no solution was found\n",
    "\n",
    "**Nesterov's accelerated gradient descent:**\n",
    "\n",
    "1. Inputs received by the function (expected: function (`f`, str), starting point (`x0`, int, float),  convergence threshold (`epsilon`, int, float), maximum number of iterations to be done (`max_iters`, int), learning rate (`eta`, int, float)\n",
    "2. Store the starting point in variables `xn` and `yn`\n",
    "3. Create a `DreamDiff` object and initialize a momentum parameter `t` at 1.0\n",
    "4. Parse the input function by calling `_parse_input`\n",
    "5. Create a list `xn_history` to store all the points that the algorithm encounters while moving towards the minimum\n",
    "6. Start looping within the range set by the user (`max_iters`)\n",
    "7. (Within the loop) Add the starting point to `xn_history`\n",
    "8. (Within the loop) Get the value and derivative at `xn`\n",
    "9. (Within the loop) Update the momentum parameter `t` using Nesterov's momentum update\n",
    "9. (Within the loop) Compute the new value of `xn` using the current value of `yn`, `eta` and derivative\n",
    "10. (Within the loop) Compute the new value of `yn` using `xn`, `new_xn`, `t` and `new_t`  \n",
    "11. (Within the loop) Check for convergence by computing if the difference between new value of `xn` from Step 9 and current value is less than `epsilon`. If convergence achieved return the value of `xn` and stop\n",
    "12. If end of the loop is reached without convergence then print out message indicating no solution was found\n",
    "\n",
    "To show this in action, let $f(x) = \\tan(\\sin(x)+3).$ A local minimum of this\n",
    "function can be found using standard gradient descent by calling `grad_descent` on it. The additional\n",
    "optional parameter, `eta`, is the learning rate and controls the step size of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solution after 15 iterations.\n",
      "Solution is: 4.712385613167822\n"
     ]
    }
   ],
   "source": [
    "f = 'tan(sin(x) + 3)'\n",
    "results = Optimize.grad_descent(f, 4, epsilon=0.00001, max_iters=500, eta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This minimum could also have been located using `nesterov_grad_descent`, which is often faster than the standard gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solution after 11 iterations.\n",
      "Solution is: 4.712385876514812\n"
     ]
    }
   ],
   "source": [
    "f = 'tan(sin(x) + 3)'\n",
    "results = Optimize.nesterov_grad_descent(f, 4, epsilon=0.00001, max_iters=500, eta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Newton's method, the gradient descent methods can also be visualized by calling `animate_grad_desc`, which produces a `matplotlib` animation of the descent algorithm. Note that the additional parameter `method` is used to specify which gradient descent method should be utilized.\n",
    "\n",
    "For example, running the commented line will produce an animation showing this descent using Nesterov's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'tan(sin(x) + 3)'\n",
    "#Optimize.animate_grad_desc(f, 4, epsilon=0.00001, max_iters=500, eta=0.1, runtime=20, method='nesterov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nesterov_animate.png](nesterov_animate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Splines\n",
    "\n",
    "Quadratic Spline Interpolation is a form of \n",
    "interpolation that uses piece-wise quadratic functions to connect \n",
    "points. Quadratic splines have the advantage of being\n",
    "low-order polynomial, and thus are much easier to calculate. \n",
    "They take in `n+1` points on a 2-dimensional plane and create a \n",
    "system of `3n` quadratic equations which can be evaluated to linear \n",
    "equations via derivatives. We use least squares to solve this system, \n",
    "and output `n` piecewise quadratic functions and their coefficients. \n",
    "\n",
    "The first `2n` quadratic equations ensure that `a*x_i^2 + b*x_i + c` crosses \n",
    "the input points before and after it. The next `n-1` equations ensure that\n",
    "the derivative of the the `i`th and `i+1`th quadratic equations match\n",
    "at each connection point from the input to ensure continuity. \n",
    "For this, `DreamDiff` is used to easily calculate \n",
    "these derivatives and values. The last equation is a default equation \n",
    "to ensure squareness of our system and sets `a_0` to 0.\n",
    "\n",
    "For example, `x = [2,3,4,5,6]` and `y = [-3,-8,-11,-14,-18]` can be defined\n",
    "as the input points as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Spline Equations: \n",
      "S_0(x) = 0.0*x^2 + -5.0*x + 7.0 for x ∈ [2, 3]\n",
      "S_1(x) = 0.6667*x^2 + -7.6667*x + 9.0 for x ∈ [3, 4]\n",
      "S_2(x) = 0.3333*x^2 + -6.0*x + 7.6667 for x ∈ [4, 5]\n",
      "S_3(x) = -0.0*x^2 + -4.0*x + 6.0 for x ∈ [5, 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = Optimize.quadratic_spline([2,3,4,5,6], [-3,-8,-11,-14,-18], plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact and inclusivity\n",
    "\n",
    "Our goal for DreamDiff is for it to live well beyond the confines of a computer science course, as all good software should. While we believe it has potential to be used in a wide range of exciting applications, it is important to also consider the broader implications of our software.\n",
    "\n",
    "Our package implements forward-mode automatic differentiation, gradient descent, Nesterov's accelerated gradient descent, Newton’s method and quadratic splines. All these tools have distinct usages, but we hope the convenience of a centralized optimization package provides some extra utility to users working on various optimization tasks in machine learning, minimization and maximization problems, and interpolation using quadratic splines.\n",
    "\n",
    "Further, we hope that the visualization and animation functionality built into our optimization functions serve as a useful educational tool, providing a more intuitive understanding of how these methods work and encouraging math and computer science education more broadly. Perhaps most importantly, we hope that by open-sourcing our software, it will be accessible to as many people as possible. We believe that open-source platforms like GitHub have inherent advantages for inclusivity, providing open access to people around the world who may want to utilize or build upon its core functionality. Nonetheless, it is important to address the more subtle yet persistent inequalities in software development and the broader field of computer science.\n",
    "\n",
    "For example, open source still requires an internet connection, which automatically excludes the 3.6 billion people across the globe (around 47% of the world's population) who do not use the internet, let alone have a computer or access to the educational resources required to learn basic STEM skills. But even in regions of the world where internet access is not an issue, historical gender imbalances have created barriers for women and people of color in computer science. Regardless of your own background, we ask that you keep these inequities in mind and consider how your own work environment can be made more inclusive, particularly for groups that face historical barriers in STEM fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "# NOT DONE YET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Hoffmann, P. H. (2016). A hitchhiker’s guide to automatic differentiation. Numerical Algorithms, 72(3), 775-811.\n",
    "2. van Merrienboer, B., Moldovan, D., & Wiltschko, A. (2018). Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming. In Advances in Neural Information Processing Systems (pp. 6256-6265).\n",
    "3. https://harvard-iacs.github.io/2020-CS107/lectures/.\n",
    "4. Griewank, A. and Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation (Vol. 105). Siam.\n",
    "5. Nocedal, J. and Wright, S., 2001. Numerical Optimization, Second Edition. Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
